{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Problem using Pyspark to work with different file format inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"10.0.2.15\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Text file and performing word count problem\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this:1\n",
      "is:3\n",
      "a:2\n",
      "sample:1\n",
      "input:1\n",
      "text:2\n",
      "file:3\n",
      "for:1\n",
      "wordcount:2\n",
      "program:2\n",
      "being:1\n",
      "implemented:1\n",
      "using:1\n",
      "pyspark:1\n",
      "will:1\n",
      "be:1\n",
      "stored:1\n",
      "on:1\n",
      "hdfs:2\n",
      "distributed:1\n",
      "system:1\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName('Firstprogram').getOrCreate()\n",
    "\n",
    "# Read the input file and create DataFrame\n",
    "df = spark.read.text(\"/home/hdoop/Documents/python/pyspark/input1.txt\")\n",
    "\n",
    "lines = df.rdd.map(lambda r:r[0])\n",
    "\n",
    "# Split lines into words and perform word count\n",
    "counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "                            .map(lambda word: (word, 1)) \\\n",
    "                            .reduceByKey(add)\n",
    "# Printing each word with its respective count\n",
    "output = counts.collect()\n",
    "\n",
    "for (word,count) in output:\n",
    "    print (\"%s:%i\" % (word,count))\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading CSV file and performing word count problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Name|\n",
      "+--------+\n",
      "| shubham|\n",
      "|  shirke|\n",
      "|shreyash|\n",
      "|   hello|\n",
      "|   world|\n",
      "|       I|\n",
      "|      am|\n",
      "|  world |\n",
      "|      to|\n",
      "|   hello|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    word|count|\n",
      "+--------+-----+\n",
      "|shreyash|    1|\n",
      "|   hello|    2|\n",
      "| shubham|    1|\n",
      "|  shirke|    1|\n",
      "|   world|    2|\n",
      "|       I|    1|\n",
      "|        |    1|\n",
      "|      am|    1|\n",
      "|      to|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "# Create SparkSession\n",
    "csv_spark = SparkSession.builder.appName(\"CSVReader\").getOrCreate()\n",
    "\n",
    "# File path\n",
    "csv_file_path = \"data_input.csv\"\n",
    "\n",
    "# Read CSV file\n",
    "df = csv_spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "text_column = 'Name'\n",
    "\n",
    "df = df.select(text_column)\n",
    "\n",
    "# Split the text into words\n",
    "words_df = df.select(explode(split(df[text_column], \" \")).alias(\"word\"))\n",
    "\n",
    "word_counts = words_df.groupBy(\"word\").count()\n",
    "\n",
    "# Show the word count results\n",
    "word_counts.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "csv_spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Json file and perfrom word count Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      "\n",
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|     apple|\n",
      "|    banana|\n",
      "|    cherry|\n",
      "|      date|\n",
      "|elderberry|\n",
      "|    banana|\n",
      "|elderberry|\n",
      "|     guaua|\n",
      "|     apple|\n",
      "|    orange|\n",
      "+----------+\n",
      "\n",
      "Output In Json Format\n",
      "{\"word\":\"guaua\",\"count\":1}\n",
      "{\"word\":\"orange\",\"count\":1}\n",
      "{\"word\":\"elderberry\",\"count\":2}\n",
      "{\"word\":\"apple\",\"count\":2}\n",
      "{\"word\":\"cherry\",\"count\":1}\n",
      "{\"word\":\"banana\",\"count\":2}\n",
      "{\"word\":\"date\",\"count\":1}\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import to_json ,explode,split\n",
    "\n",
    "\n",
    "json_spark = SparkSession.builder.appName('JsonReader').getOrCreate()\n",
    "\n",
    "json_file_path = \"json_input.json\"\n",
    "df = json_spark.read.json(json_file_path)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "text_column = 'word'\n",
    "\n",
    "df = df.select(text_column)\n",
    "\n",
    "words_df = df.select(explode(split(df[text_column], \" \")).alias(\"word\"))\n",
    "\n",
    "word_counts = words_df.groupBy(\"word\").count()\n",
    "\n",
    "\n",
    "#word_counts.show()\n",
    "output = word_counts.toJSON().collect()\n",
    "\n",
    "print(\"Output In Json Format\")\n",
    "for i in output:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n",
    "json_spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Write Operations with different file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"10.0.2.15\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/17 14:44:23 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|spark is an execu...|\n",
      "|pyspark library o...|\n",
      "|pyspark is openso...|\n",
      "|hello i am workin...|\n",
      "+--------------------+\n",
      "\n",
      "Content successfully appended to the file\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"TextFile\").getOrCreate()\n",
    "\n",
    "    text_file_path = \"hdfs://localhost:9000/read_write/input2.txt\"\n",
    "    df = spark.read.text(text_file_path)\n",
    "    df.show()\n",
    "    input_text = \"hello i am working with pyspark\"\n",
    "\n",
    "    # Append the content to the file using Hadoop FileSystem APIs\n",
    "    hadoop_fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jvm.java.net.URI(text_file_path), spark._jvm.org.apache.hadoop.conf.Configuration())\n",
    "    output_stream = hadoop_fs.append(spark._jvm.org.apache.hadoop.fs.Path(text_file_path))\n",
    "    output_stream.write(input_text.encode())\n",
    "    output_stream.close()\n",
    "\n",
    "    print(\"Content successfully appended to the file\")\n",
    "except Exception as e:\n",
    "    print(\"Error: \" + str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. CSV File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   word|\n",
      "+-------+\n",
      "|Writing|\n",
      "|   file|\n",
      "|    csv|\n",
      "|     in|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|   word|\n",
      "+-------+\n",
      "|Writing|\n",
      "|     in|\n",
      "|    csv|\n",
      "|   file|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   word|\n",
      "+-------+\n",
      "|Writing|\n",
      "|   file|\n",
      "|    csv|\n",
      "|     in|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSVFile\").getOrCreate()\n",
    "\n",
    "# Read the CSV file with inferred schema, comma separator, and header\n",
    "csv_file_path = \"hdfs://localhost:9000/read_write/data_input.csv\"\n",
    "df = spark.read.format(\"csv\").options(inferSchema=\"True\", sep=\",\", header=\"True\").load(csv_file_path)\n",
    "\n",
    "# Show the DataFrame contents\n",
    "df.show()\n",
    "\n",
    "# Split the input text into words\n",
    "input_text = \"Writing in csv file\"\n",
    "words = input_text.split()\n",
    "\n",
    "# Creating a DataFrame with words in separate rows\n",
    "input_data = [(word,) for word in words]\n",
    "input_df = spark.createDataFrame(input_data, [\"word\"])\n",
    "\n",
    "input_df.show()\n",
    "\n",
    "# Union the two DataFrames\n",
    "input_df.write.format(\"csv\").save(\"hdfs://localhost:9000/read_write/data_output.csv\",header=True)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Name|\n",
      "+--------+\n",
      "| shubham|\n",
      "|  shirke|\n",
      "|shreyash|\n",
      "|   hello|\n",
      "|   world|\n",
      "|       I|\n",
      "|      am|\n",
      "|  world |\n",
      "|      to|\n",
      "|   hello|\n",
      "+--------+\n",
      "\n",
      "+-------+\n",
      "|   word|\n",
      "+-------+\n",
      "|Writing|\n",
      "|     in|\n",
      "|    csv|\n",
      "|   file|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSVFile\").getOrCreate()\n",
    "\n",
    "# Read the CSV file with inferred schema, comma separator, and header\n",
    "csv_file_path = \"hdfs://localhost:9000/read_write/data_input.csv\"\n",
    "df = spark.read.format(\"csv\").options(inferSchema=\"True\", sep=\",\", header=\"True\").load(csv_file_path)\n",
    "\n",
    "# Show the DataFrame contents\n",
    "df.show()\n",
    "\n",
    "# Split the input text into words\n",
    "input_text = \"Writing in csv file\"\n",
    "words = input_text.split()\n",
    "\n",
    "# Creating a DataFrame with words in separate rows\n",
    "input_data = [(word,) for word in words]\n",
    "input_df = spark.createDataFrame(input_data, [\"word\"])\n",
    "\n",
    "input_df.show()\n",
    "\n",
    "# write in csv file\n",
    "input_df.write.format(\"csv\").save(\"hdfs://localhost:9000/read_write/data_input2.csv\",header=True)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Name|\n",
      "+--------+\n",
      "| shubham|\n",
      "|  shirke|\n",
      "|shreyash|\n",
      "|   hello|\n",
      "|   world|\n",
      "|       I|\n",
      "|      am|\n",
      "|  world |\n",
      "|      to|\n",
      "|   hello|\n",
      "+--------+\n",
      "\n",
      "Output In Json Format\n",
      "{\"Name\":\"shubham\"}\n",
      "{\"Name\":\"shirke\"}\n",
      "{\"Name\":\"shreyash\"}\n",
      "{\"Name\":\"hello\"}\n",
      "{\"Name\":\"world\"}\n",
      "{\"Name\":\"I\"}\n",
      "{\"Name\":\"am\"}\n",
      "{\"Name\":\"world \"}\n",
      "{\"Name\":\"to\"}\n",
      "{\"Name\":\"hello\"}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSV to JSON\").getOrCreate()\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "csv_file_path = \"hdfs://localhost:9000/read_write/data_input.csv\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file_path)\n",
    "df.show()\n",
    "# Convert the DataFrame to JSON\n",
    "json_file_path = \"hdfs://localhost:9000/read_write/data_output.json\"\n",
    "#df.write.json(json_file_path)\n",
    "\n",
    "\n",
    "# Read Json File\n",
    "json_df = spark.read.format(\"json\").load(json_file_path)\n",
    "# Display Json file\n",
    "output = json_df.toJSON().collect()\n",
    "\n",
    "print(\"Output In Json Format\")\n",
    "for i in output:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File\n",
      "+--------+\n",
      "|    Name|\n",
      "+--------+\n",
      "| shubham|\n",
      "|  shirke|\n",
      "|shreyash|\n",
      "|   hello|\n",
      "|   world|\n",
      "|       I|\n",
      "|      am|\n",
      "|  world |\n",
      "|      to|\n",
      "|   hello|\n",
      "+--------+\n",
      "\n",
      "Parquet File\n",
      "Output In Json Format\n",
      "{\"Name\":\"shubham\"}\n",
      "{\"Name\":\"shirke\"}\n",
      "{\"Name\":\"shreyash\"}\n",
      "{\"Name\":\"hello\"}\n",
      "{\"Name\":\"world\"}\n",
      "{\"Name\":\"I\"}\n",
      "{\"Name\":\"am\"}\n",
      "{\"Name\":\"world \"}\n",
      "{\"Name\":\"to\"}\n",
      "{\"Name\":\"hello\"}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSV to JSON\").getOrCreate()\n",
    "\n",
    "# Reading the CSV file into a DataFrame\n",
    "csv_file_path = \"hdfs://localhost:9000/read_write/data_input.csv\"\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file_path)\n",
    "print(\"CSV File\")\n",
    "df.show()\n",
    "\n",
    "# Converting the DataFrame to JSON\n",
    "parquet_file_path = \"hdfs://localhost:9000/read_write/data_output.parquet\"\n",
    "#df.write.parquet(parquet_file_path)\n",
    "\n",
    "\n",
    "# Read Parquet file\n",
    "print(\"Parquet File\")\n",
    "new_df = spark.read.parquet(parquet_file_path)\n",
    "output = new_df.toJSON().collect()\n",
    "\n",
    "print(\"Output In Json Format\")\n",
    "for i in output:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark ---packages org.apache.spark:spark-avro_2.12-3.3.2 pyspark-shell'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
